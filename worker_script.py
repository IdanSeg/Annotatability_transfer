#!/usr/bin/env python3

import argparse
import os
import json
import logging
import scanpy as sc

from mlp_net import train_and_evaluate_mlp
import torch
import pandas as pd

# Import your dataset classes and automation utilities
from pbmc import PBMC              # your PBMC dataset class
from anndata_manager import AnnDataManager
from annotability_automations import get_subset_composition, train_and_eval

###############################################################################
# Constants / Configuration
###############################################################################
DATASET = "PBMC_healthy"  # <--- Should be pre-annotated & preprocessed
LABEL_KEY = "cell_type"             

#TODO: currenly uses predefined parameters. Adapt to dataset-dependent parameters.
# This script expects the CSV to contain columns:
#   Train_Size, Easy, Ambiguous, Hard, Test_Indices, Run
# generated by create_comps_for_workers.

###############################################################################
# Worker function
###############################################################################
def worker_run_job(
    csv_file: str,
    row_id: int,
    device: str = "cpu",
    epoch_num: int = 25,
    batch_size: int = 64,
    model_name: str = "mlp",
    output_dir: str = "results"
):
    """
    Reads a single row from `csv_file` (pbmc_healthy_worker_jobs.csv),
    loads the PBMC dataset from PBMC_healthy_annotated.h5ad via the PBMC class,
    uses get_subset_composition(...) and AnnDataManager to create train/test subsets,
    then runs train_and_eval(...) to get test_loss.

    Finally, writes the result to a per-job JSON file.
    """

    # -------------------------------------------------------------------------
    # 1) Read the job row from CSV
    # -------------------------------------------------------------------------
    if not os.path.exists(csv_file):
        raise FileNotFoundError(f"[Worker] CSV file '{csv_file}' does not exist.")

    jobs_df = pd.read_csv(csv_file)
    if row_id < 0 or row_id >= len(jobs_df):
        raise IndexError(f"[Worker] row_id={row_id} out of range (0..{len(jobs_df)-1}).")

    job_row = jobs_df.iloc[row_id]
    train_size = int(job_row["Train_Size"])
    e = int(job_row["Easy"])
    a = int(job_row["Ambiguous"])
    h = int(job_row["Hard"])
    test_indices_str = job_row["Test_Indices"]
    run_id = int(job_row.get("Run", 1))

    logging.info(f"[Worker] row={row_id} => T={train_size}, E={e}, A={a}, H={h}, run={run_id}")

    # Parse test indices from CSV
    test_indices = test_indices_str.split(",") if isinstance(test_indices_str, str) else []
    test_indices = [idx.strip() for idx in test_indices if idx.strip()]
    test_adata = manager.subset(adata, test_indices)

    # -------------------------------------------------------------------------
    # 2) Load the PBMC dataset via the PBMC class (already annotated & preprocessed)
    # -------------------------------------------------------------------------

    # Here, dataset.adata is fully annotated & preprocessed; no need for extra steps.
    adata = sc.read(DATASET + '_annotated.h5ad')
    logging.info(f"[Worker] Loaded PBMC dataset shape: {adata.shape}")

    # -------------------------------------------------------------------------
    # 3) Initialize AnnDataManager
    # -------------------------------------------------------------------------
    manager = AnnDataManager()

    # -------------------------------------------------------------------------
    # 4) Exclude the test cells so that get_subset_composition() won't overlap
    # -------------------------------------------------------------------------
    all_obs_index = set(adata.obs.index)
    test_set = set(test_indices)
    trainable_indices = list(all_obs_index - test_set)

    if len(trainable_indices) < train_size:
        logging.warning(
            f"[Worker] Not enough trainable samples: need {train_size}, have {len(trainable_indices)}."
        )
        test_loss = None
        final_train_indices = []
    else:
        # Subset the PBMC data to only the trainable set
        adata_trainable = manager.subset(adata, trainable_indices)

        # We'll prepare a group_counts dict for get_subset_composition(...)
        group_counts_dict = {
            "Easy-to-learn": e,
            "Ambiguous": a,
            "Hard-to-learn": h
        }

        # ---------------------------------------------------------------------
        # 5) Get training indices from adata_trainable
        # ---------------------------------------------------------------------
        try:
            train_adata, train_indices_local = get_subset_composition(adata_trainable, group_counts_dict)
            # train_indices_local are row labels from the subset, i.e. subset of trainable_indices
        except Exception as ex:
            logging.error(f"[Worker] get_subset_composition failed: {ex}")


            # -----------------------------------------------------------------
            # 6) Prepare data, run train_and_eval
            # -----------------------------------------------------------------
            device_torch = torch.device(device if torch.cuda.is_available() else "cpu")
            label_encoder = manager.getLabelEncoder(adata, label_key=LABEL_KEY)

            try:
                test_loss = train_and_evaluate_mlp(
                    adata_train=train_adata,
                    adata_test=test_adata,
                    label_key=LABEL_KEY,
                    label_encoder=label_encoder,
                    num_classes=len(label_encoder.classes_),
                    epoch_num=epoch_num,
                    device=device_torch,
                    format_manager=manager,
                    batch_size=batch_size
                )
                logging.info(f"[Worker] row={row_id} => test_loss={test_loss}")
            except Exception as train_ex:
                logging.error(f"[Worker] train_and_eval failed: {train_ex}")
                test_loss = None

    # -------------------------------------------------------------------------
    # 7) Save result to JSON
    # -------------------------------------------------------------------------
    os.makedirs(output_dir, exist_ok=True)
    out_path = os.path.join(output_dir, f"results_{row_id}.json")
    result_data = {
        "row_id": row_id,
        "Train_Size": train_size,
        "Easy": e,
        "Ambiguous": a,
        "Hard": h,
        "Run": run_id,
        "Test_Indices": test_indices,
        "Train_Indices": final_train_indices,
        "Test_Loss": test_loss
    }

    try:
        with open(out_path, "w") as f:
            json.dump(result_data, f, indent=2)
        logging.info(f"[Worker] Saved results to {out_path}")
    except Exception as ex_save:
        logging.error(f"[Worker] Failed to save JSON: {ex_save}")

###############################################################################
# Main entry point
###############################################################################
def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    parser = argparse.ArgumentParser(
        description="Worker script to process a single job from pbmc_healthy_worker_jobs.csv."
    )
    parser.add_argument("--csv_file", type=str, required=True,
                        help="Path to the pbmc_healthy_worker_jobs.csv file.")
    parser.add_argument("--row_id", type=int, required=True,
                        help="0-based row index in the CSV.")
    parser.add_argument("--device", type=str, default="cpu", choices=["cpu", "cuda"],
                        help="Device to use for training (cpu or cuda).")
    parser.add_argument("--epoch_num", type=int, default=25,
                        help="Number of training epochs for train_and_eval.")
    parser.add_argument("--batch_size", type=int, default=64,
                        help="Training batch size.")
    parser.add_argument("--model_name", type=str, default="mlp",
                        help="Model runner name if you have multiple.")
    parser.add_argument("--output_dir", type=str, default="results",
                        help="Directory to save result JSON files.")

    args = parser.parse_args()

    worker_run_job(
        csv_file=args.csv_file,
        row_id=args.row_id,
        device=args.device,
        epoch_num=args.epoch_num,
        batch_size=args.batch_size,
        model_name=args.model_name,
        output_dir=args.output_dir
    )

if __name__ == "__main__":
    main()