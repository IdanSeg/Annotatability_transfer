{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:24:19.008734Z",
     "start_time": "2024-11-30T09:24:18.974756Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "print(\"start import\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print('\\n'.join(sys.path))\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import scanpy as sc\n",
    "# from sklearn.preprocessing import normalize\n",
    "# from sklearn.metrics import roc_auc_score, pairwise_distances, accuracy_score\n",
    "import torch\n",
    "# import torch.optim as optim\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# import scipy.sparse as sp\n",
    "# from numba import jit\n",
    "from Annotatability import models\n",
    "# from Annotatability import metrics\n",
    "# import scvi\n",
    "import logging\n",
    "# import squidpy as sq\n",
    "# import contextlib\n",
    "# import io\n",
    "import random\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # Added imports\n",
    "import scipy.sparse as sp\n",
    "# from sklearn.model_selection import train_test_split\n",
    "print(\"finished import\")\n",
    "logging.getLogger(\"scvi\").setLevel(logging.WARNING)\n",
    "\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 20.5\n",
    "BIGGER_SIZE = 24\n",
    "#plt.rcParams[\"font.family\"] = \"Verdana\"\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "sc.set_figure_params(scanpy=True, fontsize=20.5)\n",
    "\n",
    "# Define custom color palette\n",
    "annotation_order = ['Easy-to-learn', 'Ambiguous', 'Hard-to-learn']\n",
    "annotation_colors = ['green', 'orange', 'red']\n",
    "palette = dict(zip(annotation_order, annotation_colors))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start import\n",
      "/usr/bin/python3\n",
      "/Applications/PyCharm.app/Contents/plugins/python/helpers-pro/jupyter_debug\n",
      "/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev\n",
      "/tmp/pycharm_project_941\n",
      "/tmp/pycharm_project_941\n",
      "/usr/lib/python311.zip\n",
      "/usr/lib/python3.11\n",
      "/usr/lib/python3.11/lib-dynload\n",
      "\n",
      "/usr/local/lib/python3.11/dist-packages\n",
      "/usr/lib/python3/dist-packages\n",
      "/usr/lib/python3.11/dist-packages\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [5], line 16\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(sys\u001B[38;5;241m.\u001B[39mpath))\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# import numpy as np\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# import pandas as pd\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# import matplotlib.pyplot as plt\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# from sklearn.preprocessing import normalize\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# from sklearn.metrics import roc_auc_score, pairwise_distances, accuracy_score\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# import torch.optim as optim\u001B[39;00m\n\u001B[1;32m     18\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T09:13:28.904507Z",
     "start_time": "2024-11-30T09:13:28.864817Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# Initialize StringIO object to suppress outputs\n",
    "f = io.StringIO()\n",
    "\n",
    "def train_and_get_prob_list(adata, label_key, epoch_num, device=device, batch_size=128):\n",
    "    print('Training the model...')\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        prob_list = models.follow_training_dyn_neural_net(\n",
    "            adata,\n",
    "            label_key=label_key,\n",
    "            iterNum=epoch_num,\n",
    "            device=device,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    print('Training complete.')\n",
    "    return prob_list\n",
    "\n",
    "def calculate_confidence_and_variability(prob_list, n_obs, epoch_num):\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        all_conf, all_var = models.probability_list_to_confidence_and_var(\n",
    "            prob_list,\n",
    "            n_obs=n_obs,\n",
    "            epoch_num=epoch_num\n",
    "        )\n",
    "    return all_conf, all_var\n",
    "\n",
    "def find_cutoffs(adata, label_key, device, probability, percentile, epoch_num):\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        cutoff_conf, cutoff_var = models.find_cutoff_paramter(\n",
    "            adata,\n",
    "            label_key,\n",
    "            device,\n",
    "            probability=probability,\n",
    "            percentile=percentile,\n",
    "            epoch_num=epoch_num\n",
    "        )\n",
    "    return cutoff_conf, cutoff_var\n",
    "\n",
    "def assign_annotations(adata, all_conf, all_var, cutoff_conf, cutoff_var, annotation_col='Annotation'):\n",
    "    adata.obs[\"var\"] = all_var.detach().numpy()\n",
    "    adata.obs[\"conf\"] = all_conf.detach().numpy()\n",
    "    adata.obs['conf_binaries'] = pd.Categorical(\n",
    "        (adata.obs['conf'] > cutoff_conf) | (adata.obs['var'] > cutoff_var)\n",
    "    )\n",
    "\n",
    "    annotation_list = []\n",
    "    for i in tqdm(range(adata.n_obs), desc='Assigning annotations'):\n",
    "        if adata.obs['conf_binaries'].iloc[i]:\n",
    "            if (adata.obs['conf'].iloc[i] > 0.95) & (adata.obs['var'].iloc[i] < 0.15):\n",
    "                annotation_list.append('Easy-to-learn')\n",
    "            else:\n",
    "                annotation_list.append('Ambiguous')\n",
    "        else:\n",
    "            annotation_list.append('Hard-to-learn')\n",
    "\n",
    "    adata.obs[annotation_col] = annotation_list\n",
    "    adata.obs['Confidence'] = adata.obs['conf']\n",
    "    adata.obs['Variability'] = adata.obs['var']\n",
    "    return adata"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcontextlib\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mio\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tqdm'"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Merfish and PBMC"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load and preprocess the data\n",
    "adata_merfish = sq.datasets.merfish()\n",
    "sc.pp.normalize_per_cell(adata_merfish, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata_merfish)\n",
    "\n",
    "# Map clusters to cell types\n",
    "cell_type_mapping = {\n",
    "    'OD Mature 2': 'OD Mature',\n",
    "    'OD Immature 1': 'OD Immature',\n",
    "    'Inhibitory': 'Inhibitory',\n",
    "    'Excitatory': 'Excitatory',\n",
    "    'Microglia': 'Microglia',\n",
    "    'Astrocyte': 'Astrocyte',\n",
    "    'Endothelial 2': 'Endothelial',\n",
    "    'Endothelial 3': 'Endothelial',\n",
    "    'Endothelial 1': 'Endothelial',\n",
    "    'OD Mature 1': 'OD Mature',\n",
    "    'OD Mature 4': 'OD Mature',\n",
    "    'Pericytes': 'Pericytes',\n",
    "    'OD Mature 3': 'OD Mature',\n",
    "    'Ependymal': 'Ependymal',\n",
    "    'OD Immature 2': 'OD Immature'\n",
    "}\n",
    "adata_merfish.obs['CellType'] = adata_merfish.obs['Cell_class'].map(cell_type_mapping).fillna(adata_merfish.obs['Cell_class'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import scanpy as sc\n",
    "\n",
    "# Load the dataset\n",
    "# PBMC_full is obtained from https://cellxgene.cziscience.com/collections/03f821b4-87be-4ff4-b65a-b5fc00061da7\n",
    "# Change to your path below\n",
    "adata_full = sc.read_h5ad('PBMC_full.h5ad')\n",
    "\n",
    "# Define the list of cell types to keep (updated to match actual cell types in the dataset)\n",
    "cell_types_to_keep = [\n",
    "    'B cell',\n",
    "    'CD4-positive helper T cell',\n",
    "    'naive thymus-derived CD8-positive, alpha-beta T cell',\n",
    "    'naive thymus-derived CD4-positive, alpha-beta T cell',\n",
    "    'classical monocyte',\n",
    "    'dendritic cell',\n",
    "    'CD16-negative, CD56-bright natural killer cell',\n",
    "    'mature NK T cell'\n",
    "]\n",
    "\n",
    "# **Filter to include only healthy cells**\n",
    "if 'COVID_status' in adata_full.obs.columns:\n",
    "    if 'Healthy' in adata_full.obs['COVID_status'].unique():\n",
    "        adata_healthy = adata_full[adata_full.obs['COVID_status'] == 'Healthy'].copy()\n",
    "        print(\"Filtered healthy cells using 'COVID_status'.\")\n",
    "    else:\n",
    "        raise ValueError(\"'Healthy' label not found in 'COVID_status' column.\")\n",
    "else:\n",
    "    raise KeyError(\"'COVID_status' column not found in adata_full.obs.\")\n",
    "\n",
    "# Inspect the cell types available in the healthy dataset before filtering\n",
    "print(\"Available cell types in the healthy dataset before filtering:\")\n",
    "print(adata_healthy.obs['cell_type'].unique())\n",
    "\n",
    "# **Filter the data to include only the selected cell types**\n",
    "adata_healthy = adata_healthy[adata_healthy.obs['cell_type'].isin(cell_types_to_keep)].copy()\n",
    "\n",
    "# **Throw an error if no healthy cells are found after cell type filtering**\n",
    "if adata_healthy.n_obs == 0:\n",
    "    raise ValueError(\"No healthy cells found after filtering for the selected cell types.\")\n",
    "\n",
    "# Inspect the cell types available after filtering\n",
    "print(\"Available cell types in the healthy dataset after filtering:\")\n",
    "print(adata_healthy.obs['cell_type'].unique())\n",
    "\n",
    "# Normalize and log-transform the data\n",
    "sc.pp.normalize_total(adata_healthy, target_sum=1e4)\n",
    "sc.pp.log1p(adata_healthy)\n",
    "\n",
    "pbmc = adata_healthy\n",
    "\n",
    "# Final verification\n",
    "print(f\"Final number of cells after all filtering: {pbmc.n_obs}\")\n",
    "print(f\"Final number of genes: {pbmc.n_vars}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"MERFISH\")\n",
    "prob_list_merfish = train_and_get_prob_list(adata_merfish, label_key='CellType', epoch_num=150, device=device, batch_size=64)\n",
    "\n",
    "# Train and get confidence and variability of MERFISH\n",
    "all_conf_merfish, all_var_merfish = calculate_confidence_and_variability(prob_list_merfish, n_obs=adata_merfish.n_obs, epoch_num=150)\n",
    "\n",
    "# Find cutoffs of MERFISH\n",
    "conf_cutoff_merfish, var_cutoff_merfish = find_cutoffs(adata_merfish, 'CellType', device, probability=0.1, percentile=90, epoch_num=150)\n",
    "\n",
    "# Assign annotations\n",
    "adata_merfish = assign_annotations(adata_merfish, all_conf_merfish, all_var_merfish, conf_cutoff_merfish, var_cutoff_merfish, annotation_col='Annotation')\n",
    "\n",
    "# Count the number of cells in each group\n",
    "group_counts_merfish = adata_merfish.obs['Annotation'].value_counts()\n",
    "\n",
    "print(group_counts_merfish)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"PBMC\")\n",
    "# Train the model and get probability list for PBMC dataset\n",
    "prob_list_pbmc = train_and_get_prob_list(pbmc, label_key='cell_type', epoch_num=150, device=device, batch_size=64)\n",
    "\n",
    "# Calculate confidence and variability for PBMC\n",
    "all_conf_pbmc, all_var_pbmc = calculate_confidence_and_variability(prob_list_pbmc, n_obs=pbmc.n_obs, epoch_num=150)\n",
    "\n",
    "# Find cutoffs for PBMC\n",
    "conf_cutoff_pbmc, var_cutoff_pbmc = find_cutoffs(pbmc, 'cell_type', device, probability=0.1, percentile=90, epoch_num=150)\n",
    "\n",
    "# Assign annotations\n",
    "pbmc = assign_annotations(pbmc, all_conf_pbmc, all_var_pbmc, conf_cutoff_pbmc, var_cutoff_pbmc, annotation_col='Annotation')\n",
    "\n",
    "# Count the number of cells in each group\n",
    "group_counts_pbmc = pbmc.obs['Annotation'].value_counts()\n",
    "\n",
    "print(group_counts_pbmc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Assuming 'group_counts_merfish' is a pandas Series with annotations as indices\n",
    "group_counts_merfish = adata_merfish.obs['Annotation'].value_counts()\n",
    "\n",
    "# Assign counts to E_merfish, A_merfish, H_merfish\n",
    "E_merfish = group_counts_merfish.get('Easy-to-learn', 0)\n",
    "A_merfish = group_counts_merfish.get('Ambiguous', 0)\n",
    "H_merfish = group_counts_merfish.get('Hard-to-learn', 0)\n",
    "\n",
    "# Get the indices of each group\n",
    "easy_indices_merfish = adata_merfish.obs.index[adata_merfish.obs['Annotation'] == 'Easy-to-learn'].tolist()\n",
    "ambiguous_indices_merfish = adata_merfish.obs.index[adata_merfish.obs['Annotation'] == 'Ambiguous'].tolist()\n",
    "hard_indices_merfish = adata_merfish.obs.index[adata_merfish.obs['Annotation'] == 'Hard-to-learn'].tolist()\n",
    "\n",
    "# Fit LabelEncoder on the entire dataset labels\n",
    "global_label_encoder_merfish = LabelEncoder()\n",
    "global_label_encoder_merfish.fit(adata_merfish.obs['CellType'])\n",
    "num_classes_merfish = len(global_label_encoder_merfish.classes_)\n",
    "\n",
    "# Verify the counts match E_merfish, A_merfish, H_merfish\n",
    "print(\"MERFISH:\")\n",
    "print(f\"Number of Easy-to-learn samples: {len(easy_indices_merfish)}\")\n",
    "print(f\"Number of Ambiguous samples: {len(ambiguous_indices_merfish)}\")\n",
    "print(f\"Number of Hard-to-learn samples: {len(hard_indices_merfish)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "subset_size = 100000\n",
    "pbmc = pbmc[pbmc.obs.sample(n=subset_size, random_state=42).index].copy()\n",
    "\n",
    "# Assuming 'group_counts_pbmc' is a pandas Series with annotations as indices\n",
    "group_counts_pbmc = pbmc.obs['Annotation'].value_counts()\n",
    "\n",
    "# Assign counts to E_pbmc, A_pbmc, H_pbmc\n",
    "E_pbmc = group_counts_pbmc.get('Easy-to-learn', 0)\n",
    "A_pbmc = group_counts_pbmc.get('Ambiguous', 0)\n",
    "H_pbmc = group_counts_pbmc.get('Hard-to-learn', 0)\n",
    "\n",
    "# Get the indices of each group\n",
    "easy_indices_pbmc = pbmc.obs.index[pbmc.obs['Annotation'] == 'Easy-to-learn'].tolist()\n",
    "ambiguous_indices_pbmc = pbmc.obs.index[pbmc.obs['Annotation'] == 'Ambiguous'].tolist()\n",
    "hard_indices_pbmc = pbmc.obs.index[pbmc.obs['Annotation'] == 'Hard-to-learn'].tolist()\n",
    "\n",
    "# Fit LabelEncoder on the entire dataset labels\n",
    "global_label_encoder_pbmc = LabelEncoder()\n",
    "global_label_encoder_pbmc.fit(pbmc.obs['cell_type'])\n",
    "num_classes_pbmc = len(global_label_encoder_pbmc.classes_)\n",
    "\n",
    "# Verify the counts match E_pbmc, A_pbmc, H_pbmc\n",
    "print(\"PBMC:\")\n",
    "print(f\"Number of Easy-to-learn samples: {len(easy_indices_pbmc)}\")\n",
    "print(f\"Number of Ambiguous samples: {len(ambiguous_indices_pbmc)}\")\n",
    "print(f\"Number of Hard-to-learn samples: {len(hard_indices_pbmc)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(BaseNet, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "class Net(BaseNet):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        layer_sizes = [input_size, int(input_size / 2), int(input_size / 4), output_size]\n",
    "        super(Net, self).__init__(layer_sizes)\n",
    "\n",
    "def one_hot_encode(labels, label_encoder):\n",
    "    values = np.array(labels)\n",
    "    integer_encoded = label_encoder.transform(values)\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded\n",
    "\n",
    "def is_scipy_cs_sparse(matrix):\n",
    "    return sp.issparse(matrix) and matrix.getformat() == 'csr'\n",
    "\n",
    "def train_and_evaluate_model(\n",
    "    adata_train, \n",
    "    adata_test, \n",
    "    label_key, \n",
    "    label_encoder, \n",
    "    num_classes,      \n",
    "    epoch_num, \n",
    "    device,         \n",
    "    batch_size=128\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a neural network model on the provided training and testing data.\n",
    "    \n",
    "    Parameters:\n",
    "    - adata_train (AnnData): Training dataset.\n",
    "    - adata_test (AnnData): Testing dataset.\n",
    "    - label_key (str): Key in adata.obs that contains the labels.\n",
    "    - label_encoder (LabelEncoder): Fitted LabelEncoder instance.\n",
    "    - num_classes (int): Number of unique classes in the dataset.\n",
    "    - epoch_num (int): Number of training epochs.\n",
    "    - device (str or torch.device): Device to run the training on ('cpu' or 'cuda').\n",
    "    - batch_size (int): Batch size for training.\n",
    "    \n",
    "    Returns:\n",
    "    - test_loss (float): Loss on the test dataset.\n",
    "    \"\"\"\n",
    "    def one_hot_encode(labels, label_encoder):\n",
    "        values = np.array(labels)\n",
    "        integer_encoded = label_encoder.transform(values)\n",
    "        onehot_encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "        return onehot_encoded\n",
    "\n",
    "    def is_scipy_cs_sparse(matrix):\n",
    "        return sp.issparse(matrix) and matrix.getformat() == 'csr'\n",
    "    \n",
    "    # Encode labels using the provided label encoder\n",
    "    one_hot_label_train = one_hot_encode(adata_train.obs[label_key], label_encoder=label_encoder)\n",
    "    one_hot_label_test = one_hot_encode(adata_test.obs[label_key], label_encoder=label_encoder)\n",
    "\n",
    "    # Initialize the neural network\n",
    "    net = Net(adata_train.X.shape[1], output_size=num_classes)\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Prepare training data\n",
    "    if is_scipy_cs_sparse(adata_train.X):\n",
    "        x_train = adata_train.X.toarray()\n",
    "    else:\n",
    "        x_train = np.array(adata_train.X)\n",
    "    tensor_x_train = torch.Tensor(x_train).to(device)\n",
    "    tensor_y_train = torch.LongTensor(np.argmax(one_hot_label_train, axis=1)).to(device)\n",
    "    train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Prepare test data\n",
    "    if is_scipy_cs_sparse(adata_test.X):\n",
    "        x_test = adata_test.X.toarray()\n",
    "    else:\n",
    "        x_test = np.array(adata_test.X)\n",
    "    tensor_x_test = torch.Tensor(x_test).to(device)\n",
    "    tensor_y_test = torch.LongTensor(np.argmax(one_hot_label_test, axis=1)).to(device)\n",
    "\n",
    "    # Train the network\n",
    "    net.train()\n",
    "    for epoch in range(epoch_num):\n",
    "        for inputs, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = net(tensor_x_test)\n",
    "        test_loss = criterion(outputs, tensor_y_test).item()\n",
    "\n",
    "    return test_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def find_optimal_compositions(\n",
    "    dataset_name,\n",
    "    adata,\n",
    "    label_key,\n",
    "    group_counts,\n",
    "    easy_indices,\n",
    "    ambiguous_indices,\n",
    "    hard_indices,\n",
    "    label_encoder,\n",
    "    num_classes,\n",
    "    train_sizes,\n",
    "    repeats_per_size,\n",
    "    csv_file,\n",
    "    device,\n",
    "    epoch_num=30,\n",
    "    batch_size=64\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the training and evaluation experiment for a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_name (str): Name identifier for the dataset (e.g., 'merfish', 'pbmc').\n",
    "    - adata (AnnData): The dataset to process.\n",
    "    - label_key (str): The key in adata.obs that contains the labels.\n",
    "    - group_counts (dict): Dictionary containing counts for 'Easy-to-learn', 'Ambiguous', 'Hard-to-learn'.\n",
    "    - easy_indices (list): List of indices for 'Easy-to-learn' samples.\n",
    "    - ambiguous_indices (list): List of indices for 'Ambiguous' samples.\n",
    "    - hard_indices (list): List of indices for 'Hard-to-learn' samples.\n",
    "    - label_encoder (LabelEncoder): Fitted LabelEncoder instance.\n",
    "    - num_classes (int): Number of unique classes in the dataset.\n",
    "    - train_sizes (list of int): List of training set sizes to experiment with.\n",
    "    - repeats_per_size (int): Number of repeats for each training size.\n",
    "    - csv_file (str): Filename to save/load the results.\n",
    "    - device (torch.device): The device to run the training on ('cpu' or 'cuda').\n",
    "    - epoch_num (int): Number of training epochs.\n",
    "    - batch_size (int): Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "    - best_compositions (dict): Dictionary containing the best compositions and their corresponding test losses.\n",
    "    \"\"\"\n",
    "    # Load existing results from CSV or create an empty DataFrame\n",
    "    try:\n",
    "        results_df = pd.read_csv(csv_file)\n",
    "    except FileNotFoundError:\n",
    "        # Include 'Train_Indices' and 'Test_Indices' columns\n",
    "        results_df = pd.DataFrame(columns=['Train_Size', 'Easy', 'Ambiguous', 'Hard', 'Test_Loss', 'Train_Indices', 'Test_Indices'])\n",
    "\n",
    "    # Convert the 'Train_Size' column to a dictionary with counts for faster lookup\n",
    "    existing_counts = results_df['Train_Size'].value_counts().to_dict()\n",
    "\n",
    "    best_compositions = {}\n",
    "\n",
    "    for T in train_sizes:\n",
    "        current_runs = existing_counts.get(T, 0)\n",
    "        runs_needed = repeats_per_size - current_runs\n",
    "\n",
    "        if runs_needed <= 0:\n",
    "            # Use existing entries\n",
    "            existing_rows = results_df[results_df['Train_Size'] == T]\n",
    "            for idx, row in existing_rows.iterrows():\n",
    "                easy = row['Easy']\n",
    "                ambiguous = row['Ambiguous']\n",
    "                hard = row['Hard']\n",
    "                test_loss = row['Test_Loss']\n",
    "                train_indices_str = row.get('Train_Indices', None)\n",
    "                test_indices_str = row.get('Test_Indices', None)\n",
    "                print(\n",
    "                    f\"Using cached result for {dataset_name} Train_Size={T}: Easy={easy}, Ambiguous={ambiguous}, Hard={hard}, Test Loss={test_loss}\"\n",
    "                )\n",
    "                # Store the cached results\n",
    "                if T not in best_compositions:\n",
    "                    best_compositions[T] = []\n",
    "                best_compositions[T].append({\n",
    "                    'composition': (easy, ambiguous, hard),\n",
    "                    'Test_Loss': test_loss,\n",
    "                    'Train_Indices': train_indices_str,\n",
    "                    'Test_Indices': test_indices_str\n",
    "                })\n",
    "            continue  # Skip computation for this T as all repeats are already done\n",
    "\n",
    "        else:\n",
    "            print(f\"\\nProcessing {dataset_name} train dataset size: {T} (Run {current_runs + 1} to {repeats_per_size})\")\n",
    "\n",
    "            # Calculate test size (25% of train size)\n",
    "            test_size = int(0.25 * T)\n",
    "            total_size = T + test_size\n",
    "            print(f\"Total dataset size (Train + Test): {total_size} (Train: {T}, Test: {test_size})\")\n",
    "\n",
    "            # Select the test indices once per dataset size\n",
    "            all_indices = adata.obs.index.tolist()\n",
    "            # Ensure we have enough samples for test set\n",
    "            if len(all_indices) < test_size:\n",
    "                print(f\"Not enough samples for Test Size={test_size} at Train_Size={T}\")\n",
    "                continue  # Skip if not enough samples\n",
    "\n",
    "            # Randomly sample test_size samples for the test set\n",
    "            test_indices = random.sample(all_indices, test_size)\n",
    "\n",
    "            # Define step size as a function of T\n",
    "            step_size = max(1, T // 100)\n",
    "\n",
    "            # Generate compositions summing up to T (train size)\n",
    "            compositions = []\n",
    "            E = group_counts.get('Easy-to-learn', 0)\n",
    "            A = group_counts.get('Ambiguous', 0)\n",
    "            H = group_counts.get('Hard-to-learn', 0)\n",
    "            for e in range(0, min(T, E) + 1, step_size):\n",
    "                for a in range(0, min(T - e, A) + 1, step_size):\n",
    "                    h = T - e - a\n",
    "                    if h >= 0 and h <= H:\n",
    "                        compositions.append((e, a, h))\n",
    "            if not compositions:\n",
    "                print(f\"No valid compositions for Train Size={T}\")\n",
    "                # Save an entry indicating no valid compositions\n",
    "                new_row = {\n",
    "                    'Train_Size': T,\n",
    "                    'Easy': None,\n",
    "                    'Ambiguous': None,\n",
    "                    'Hard': None,\n",
    "                    'Test_Loss': None,\n",
    "                    'Train_Indices': None,\n",
    "                    'Test_Indices': ','.join(map(str, test_indices))\n",
    "                }\n",
    "                new_row_df = pd.DataFrame([new_row])\n",
    "                results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "                results_df.to_csv(csv_file, index=False)\n",
    "                continue\n",
    "\n",
    "            print(f\"Total compositions for Train Size={T}: {len(compositions)}\")\n",
    "\n",
    "            for run in range(current_runs + 1, repeats_per_size + 1):\n",
    "                print(f\"\\n--- Run {run} for Train_Size={T} ---\")\n",
    "\n",
    "                min_loss = float('inf')\n",
    "                best_comp = None\n",
    "                best_train_indices = None\n",
    "\n",
    "                # For each composition, train and get test loss\n",
    "                for comp in tqdm(compositions, desc=f\"Testing compositions for Train Size={T} - Run {run}\"):\n",
    "                    e, a, h = comp\n",
    "                    # Ensure not exceeding group counts\n",
    "                    if e > E or a > A or h > H:\n",
    "                        continue  # Invalid composition\n",
    "\n",
    "                    # Ensure we have enough samples in each group\n",
    "                    if len(easy_indices) < e or len(ambiguous_indices) < a or len(hard_indices) < h:\n",
    "                        continue  # Skip if not enough samples\n",
    "\n",
    "                    # Randomly sample e, a, h samples from each group for training\n",
    "                    available_easy = list(set(easy_indices) - set(test_indices))\n",
    "                    available_ambiguous = list(set(ambiguous_indices) - set(test_indices))\n",
    "                    available_hard = list(set(hard_indices) - set(test_indices))\n",
    "\n",
    "                    if len(available_easy) < e or len(available_ambiguous) < a or len(available_hard) < h:\n",
    "                        continue  # Not enough samples after excluding test set\n",
    "\n",
    "                    train_easy_indices = random.sample(available_easy, e) if e > 0 else []\n",
    "                    train_ambiguous_indices = random.sample(available_ambiguous, a) if a > 0 else []\n",
    "                    train_hard_indices = random.sample(available_hard, h) if h > 0 else []\n",
    "                    train_indices = train_easy_indices + train_ambiguous_indices + train_hard_indices\n",
    "\n",
    "                    # Ensure total train samples equal T\n",
    "                    if len(train_indices) != T:\n",
    "                        continue  # Skip if train size mismatch\n",
    "\n",
    "                    # Create training and testing datasets\n",
    "                    adata_train = adata[train_indices].copy()\n",
    "                    adata_test = adata[test_indices].copy()\n",
    "\n",
    "                    # Train and get test loss\n",
    "                    test_loss = train_and_evaluate_model(\n",
    "                        adata_train=adata_train, \n",
    "                        adata_test=adata_test, \n",
    "                        label_key=label_key, \n",
    "                        label_encoder=label_encoder,\n",
    "                        num_classes=num_classes,    # Added this line\n",
    "                        epoch_num=epoch_num, \n",
    "                        device=device, \n",
    "                        batch_size=batch_size\n",
    "                    )\n",
    "\n",
    "                    # Update minimum loss and best composition\n",
    "                    if test_loss < min_loss:\n",
    "                        min_loss = test_loss\n",
    "                        best_comp = comp\n",
    "                        best_train_indices = train_indices.copy()\n",
    "\n",
    "                if best_comp is not None:\n",
    "                    easy, ambiguous, hard = best_comp\n",
    "                    print(\n",
    "                        f\"Best composition for {dataset_name} Train_Size={T} (Run {run}): Easy={easy}, Ambiguous={ambiguous}, Hard={hard}, Test Loss={min_loss}\"\n",
    "                    )\n",
    "\n",
    "                    # Append to best_compositions\n",
    "                    if T not in best_compositions:\n",
    "                        best_compositions[T] = []\n",
    "                    best_compositions[T].append({\n",
    "                        'composition': best_comp,\n",
    "                        'Test_Loss': min_loss,\n",
    "                        'Train_Indices': best_train_indices,\n",
    "                        'Test_Indices': test_indices  # Same test_indices for all runs of this T\n",
    "                    })\n",
    "\n",
    "                    # Save the result to the DataFrame and CSV\n",
    "                    new_row = {\n",
    "                        'Train_Size': T,\n",
    "                        'Easy': easy,\n",
    "                        'Ambiguous': ambiguous,\n",
    "                        'Hard': hard,\n",
    "                        'Test_Loss': min_loss,\n",
    "                        'Train_Indices': ','.join(map(str, best_train_indices)),\n",
    "                        'Test_Indices': ','.join(map(str, test_indices))\n",
    "                    }\n",
    "                    new_row_df = pd.DataFrame([new_row])\n",
    "                    results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "                    results_df.to_csv(csv_file, index=False)\n",
    "                else:\n",
    "                    print(f\"No valid compositions found for {dataset_name} Train_Size={T} (Run {run})\")\n",
    "                    # Save an entry indicating no valid compositions\n",
    "                    new_row = {\n",
    "                        'Train_Size': T,\n",
    "                        'Easy': None,\n",
    "                        'Ambiguous': None,\n",
    "                        'Hard': None,\n",
    "                        'Test_Loss': None,\n",
    "                        'Train_Indices': None,\n",
    "                        'Test_Indices': ','.join(map(str, test_indices))\n",
    "                    }\n",
    "                    new_row_df = pd.DataFrame([new_row])\n",
    "                    results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "                    results_df.to_csv(csv_file, index=False)\n",
    "\n",
    "'''\n",
    "    - **Workflow:**\n",
    "        1. **Load Existing Results:** Checks if the CSV file exists to load previous results; otherwise, initializes an empty DataFrame.\n",
    "        2. **Iterate Over Training Sizes:** For each specified training size, it checks how many runs have been completed and determines the remaining runs needed.\n",
    "        3. **Sampling Test Indices:** Randomly selects test indices ensuring reproducibility and avoiding overlaps with training data.\n",
    "        4. **Generate Compositions:** Creates possible compositions of 'Easy', 'Ambiguous', and 'Hard' samples that sum up to the training size.\n",
    "        5. **Training Loop:** For each run and each possible composition, it trains the model and records the test loss.\n",
    "        6. **Logging and Saving Results:** Updates the `best_compositions` dictionary and saves the results to the CSV file.\n",
    "\n",
    "'''"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define device globally\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define dataset configurations\n",
    "datasets = {\n",
    "    'merfish': {\n",
    "        'adata': adata_merfish,\n",
    "        'label_key': 'CellType',\n",
    "        'annotation_col': 'Annotation',\n",
    "        'csv_file': 'optimal_compositions_detailed.csv'\n",
    "    },\n",
    "    'pbmc': {\n",
    "        'adata': pbmc,\n",
    "        'label_key': 'cell_type',\n",
    "        'annotation_col': 'Annotation',\n",
    "        'csv_file': 'optimal_compositions_pbmc.csv'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize label encoders and calculate number of classes\n",
    "for name, config in datasets.items():\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(config['adata'].obs[config['label_key']])\n",
    "    config['label_encoder'] = label_encoder\n",
    "    config['num_classes'] = len(label_encoder.classes_)\n",
    "\n",
    "    # Extract group counts\n",
    "    group_counts = config['adata'].obs[config['annotation_col']].value_counts().to_dict()\n",
    "    config['group_counts'] = group_counts\n",
    "\n",
    "    # Extract indices for each group\n",
    "    config['easy_indices'] = config['adata'].obs.index[config['adata'].obs[config['annotation_col']] == 'Easy-to-learn'].tolist()\n",
    "    config['ambiguous_indices'] = config['adata'].obs.index[config['adata'].obs[config['annotation_col']] == 'Ambiguous'].tolist()\n",
    "    config['hard_indices'] = config['adata'].obs.index[config['adata'].obs[config['annotation_col']] == 'Hard-to-learn'].tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define the number of repeats per train size and train sizes\n",
    "repeats_per_size = 12\n",
    "train_sizes = [500, 1750, 3000]\n",
    "\n",
    "for name, config in datasets.items():\n",
    "    print(f\"\\n==================== Starting Experiment for {name.upper()} ====================\")\n",
    "    find_optimal_compositions(\n",
    "        dataset_name=name,\n",
    "        adata=config['adata'],\n",
    "        label_key=config['label_key'],\n",
    "        group_counts=config['group_counts'],\n",
    "        easy_indices=config['easy_indices'],\n",
    "        ambiguous_indices=config['ambiguous_indices'],\n",
    "        hard_indices=config['hard_indices'],\n",
    "        label_encoder=config['label_encoder'],\n",
    "        num_classes=config['num_classes'],\n",
    "        train_sizes=train_sizes,\n",
    "        repeats_per_size=repeats_per_size,\n",
    "        csv_file=config['csv_file'],\n",
    "        device=device,\n",
    "        epoch_num=30,\n",
    "        batch_size=64\n",
    "    )\n",
    "    print(f\"==================== Completed Experiment for {name.upper()} ====================\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the compositions from the CSV file\n",
    "csv_file = 'optimal_compositions_detailed.csv'\n",
    "try:\n",
    "    results_df = pd.read_csv(csv_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"CSV file '{csv_file}' not found.\")\n",
    "    results_df = pd.DataFrame(columns=['Train_Size', 'Easy', 'Ambiguous', 'Hard', 'Test_Loss'])\n",
    "\n",
    "# Filter out rows with missing compositions\n",
    "results_df = results_df.dropna(subset=['Easy', 'Ambiguous', 'Hard'])\n",
    "\n",
    "# Convert counts to floats and Train_Size to int\n",
    "results_df['Easy'] = results_df['Easy'].astype(float)\n",
    "results_df['Ambiguous'] = results_df['Ambiguous'].astype(float)\n",
    "results_df['Hard'] = results_df['Hard'].astype(float)\n",
    "results_df['Train_Size'] = results_df['Train_Size'].astype(int)\n",
    "\n",
    "# Check if all train sizes have the same number of runs\n",
    "counts_per_size = results_df['Train_Size'].value_counts()\n",
    "if counts_per_size.nunique() != 1:\n",
    "    print(\"Warning: Not all train sizes have the same number of rows in the CSV for each train size.\")\n",
    "\n",
    "# Calculate total and proportions for each row\n",
    "results_df['Total'] = results_df['Easy'] + results_df['Ambiguous'] + results_df['Hard']\n",
    "results_df['Proportion_Easy'] = results_df['Easy'] / results_df['Total']\n",
    "results_df['Proportion_Ambiguous'] = results_df['Ambiguous'] / results_df['Total']\n",
    "results_df['Proportion_Hard'] = results_df['Hard'] / results_df['Total']\n",
    "\n",
    "# Group by Train_Size and calculate mean and standard deviation of proportions\n",
    "grouped = results_df.groupby('Train_Size').agg({\n",
    "    'Proportion_Easy': ['mean', 'std'],\n",
    "    'Proportion_Ambiguous': ['mean', 'std'],\n",
    "    'Proportion_Hard': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "grouped.columns = ['Train_Size',\n",
    "                   'Proportion_Easy_mean', 'Proportion_Easy_std',\n",
    "                   'Proportion_Ambiguous_mean', 'Proportion_Ambiguous_std',\n",
    "                   'Proportion_Hard_mean', 'Proportion_Hard_std']\n",
    "\n",
    "# Ensure that the mean proportions sum to 1 (optional assertion)\n",
    "assert np.allclose(grouped[['Proportion_Easy_mean', 'Proportion_Ambiguous_mean', 'Proportion_Hard_mean']].sum(axis=1), 1), \"Mean proportions do not sum to 1.\"\n",
    "\n",
    "# Prepare data for plotting\n",
    "train_sizes = grouped['Train_Size'].values\n",
    "proportion_e_mean = grouped['Proportion_Easy_mean'].values\n",
    "proportion_a_mean = grouped['Proportion_Ambiguous_mean'].values\n",
    "proportion_h_mean = grouped['Proportion_Hard_mean'].values\n",
    "proportion_e_std = grouped['Proportion_Easy_std'].values\n",
    "proportion_a_std = grouped['Proportion_Ambiguous_std'].values\n",
    "proportion_h_std = grouped['Proportion_Hard_std'].values\n",
    "\n",
    "# Verify that all arrays have the same length\n",
    "array_lengths = [len(train_sizes), len(proportion_e_mean), len(proportion_a_mean), len(proportion_h_mean),\n",
    "                len(proportion_e_std), len(proportion_a_std), len(proportion_h_std)]\n",
    "if len(set(array_lengths)) != 1:\n",
    "    raise ValueError(f\"Array length mismatch: {array_lengths}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plotting grouped bar chart with error bars (variance) without percentage labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for plotting\n",
    "# (Assuming 'grouped' DataFrame is already prepared in the preprocessing cell)\n",
    "train_sizes = grouped['Train_Size'].values\n",
    "proportion_e_mean = grouped['Proportion_Easy_mean'].values\n",
    "proportion_a_mean = grouped['Proportion_Ambiguous_mean'].values\n",
    "proportion_h_mean = grouped['Proportion_Hard_mean'].values\n",
    "proportion_e_std = grouped['Proportion_Easy_std'].values\n",
    "proportion_a_std = grouped['Proportion_Ambiguous_std'].values\n",
    "proportion_h_std = grouped['Proportion_Hard_std'].values\n",
    "\n",
    "# Set up the plot for Grouped Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(train_sizes))\n",
    "\n",
    "# Plot each category as separate bars with error bars\n",
    "bars_easy = ax.bar(index - bar_width, proportion_e_mean, bar_width, yerr=proportion_e_std,\n",
    "                  label='Easy-to-learn', color='green', capsize=5)\n",
    "bars_ambiguous = ax.bar(index, proportion_a_mean, bar_width, yerr=proportion_a_std,\n",
    "                        label='Ambiguous', color='orange', capsize=5)\n",
    "bars_hard = ax.bar(index + bar_width, proportion_h_mean, bar_width, yerr=proportion_h_std,\n",
    "                  label='Hard-to-learn', color='red', capsize=5)\n",
    "\n",
    "# Customize the axes\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([str(size) for size in train_sizes], rotation=45)\n",
    "ax.set_ylabel('Average Proportion')\n",
    "ax.set_xlabel('Train Set Size')\n",
    "ax.set_title('Optimal Composition of Train Set Samples with Standard Deviation')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plotting stacked bar chart without error bars but with percentage labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for plotting\n",
    "# (Assuming 'grouped' DataFrame is already prepared in the preprocessing cell)\n",
    "# train_sizes, proportion_e_mean, proportion_a_mean, proportion_h_mean are already defined\n",
    "\n",
    "# Set up the plot for Stacked Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "bar_width = 0.6\n",
    "index = np.arange(len(train_sizes))\n",
    "\n",
    "# Plot Easy-to-learn segment\n",
    "bars_easy = ax.bar(index, proportion_e_mean, bar_width, label='Easy-to-learn', color='green')\n",
    "\n",
    "# Plot Ambiguous segment on top of Easy-to-learn\n",
    "bars_ambiguous = ax.bar(index, proportion_a_mean, bar_width, bottom=proportion_e_mean,\n",
    "                        label='Ambiguous', color='orange')\n",
    "\n",
    "# Plot Hard-to-learn segment on top of Ambiguous\n",
    "bars_hard = ax.bar(index, proportion_h_mean, bar_width, bottom=proportion_e_mean + proportion_a_mean,\n",
    "                  label='Hard-to-learn', color='red')\n",
    "\n",
    "# Add percentage labels\n",
    "for i in range(len(train_sizes)):\n",
    "    # Easy-to-learn\n",
    "    if proportion_e_mean[i] > 0.05:\n",
    "        ax.text(index[i], proportion_e_mean[i]/2,\n",
    "                f\"{proportion_e_mean[i]*100:.1f}%\", ha='center', va='center',\n",
    "                color='white', fontsize=10)\n",
    "    # Ambiguous\n",
    "    if proportion_a_mean[i] > 0.05:\n",
    "        ax.text(index[i], proportion_e_mean[i] + proportion_a_mean[i]/2,\n",
    "                f\"{proportion_a_mean[i]*100:.1f}%\", ha='center', va='center',\n",
    "                color='white', fontsize=10)\n",
    "    # Hard-to-learn\n",
    "    if proportion_h_mean[i] > 0.05:\n",
    "        ax.text(index[i], proportion_e_mean[i] + proportion_a_mean[i] + proportion_h_mean[i]/2,\n",
    "                f\"{proportion_h_mean[i]*100:.1f}%\", ha='center', va='center',\n",
    "                color='white', fontsize=10)\n",
    "\n",
    "# Customize the axes\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([str(size) for size in train_sizes], rotation=45)\n",
    "ax.set_ylabel('Average Proportion')\n",
    "ax.set_xlabel('Train Set Size')\n",
    "ax.set_title('Optimal Composition of Train Set Samples')\n",
    "ax.legend(loc='lower left')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# New cell to calculate compositions using top T samples with highest confidence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the optimal_compositions_detailed.csv to get the test indices used previously\n",
    "best_comp_df = pd.read_csv('optimal_compositions_detailed.csv')\n",
    "\n",
    "# Initialize a new DataFrame to store the results\n",
    "high_conf_df = pd.DataFrame(columns=['Train_Size', 'Train_Indices', 'Test_Indices', 'Test_Loss'])\n",
    "\n",
    "# Sort the samples by confidence scores in descending order\n",
    "# Assuming 'conf' is the column in adata_merfish.obs that contains confidence scores\n",
    "sorted_conf = adata_merfish.obs.sort_values(by='conf', ascending=False)\n",
    "\n",
    "# For each train size\n",
    "for T in train_sizes:\n",
    "    print(f\"\\nProcessing high-confidence composition for Train_Size={T}\")\n",
    "    \n",
    "    # Get the entries for Train_Size=T to retrieve the test indices\n",
    "    size_df = best_comp_df[best_comp_df['Train_Size'] == T]\n",
    "    \n",
    "    if size_df.empty:\n",
    "        print(f\"No entries found for Train_Size={T} in optimal_compositions_detailed.csv. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Assuming all entries for a given Train_Size use the same Test_Indices\n",
    "    # Fetch unique Test_Indices for Train_Size=T\n",
    "    unique_test_indices = size_df['Test_Indices'].unique()\n",
    "    \n",
    "    if len(unique_test_indices) != 1:\n",
    "        print(f\"Multiple test sets found for Train_Size={T}. Using the first one.\")\n",
    "    \n",
    "    test_indices_str = unique_test_indices[0]\n",
    "    \n",
    "    if pd.isnull(test_indices_str):\n",
    "        print(f\"No test indices found for Train_Size={T}. Skipping.\")\n",
    "        continue  # Skip if no test indices\n",
    "    \n",
    "    test_indices = test_indices_str.split(',')\n",
    "    \n",
    "    # Select the top T samples with highest confidence as the training set\n",
    "    # Ensure no overlap with test_indices\n",
    "    top_conf_indices = sorted_conf.index.difference(test_indices)[:T].tolist()\n",
    "    \n",
    "    if len(top_conf_indices) < T:\n",
    "        print(f\"Not enough available samples to select top {T} without overlapping with test set.\")\n",
    "        print(f\"Available samples: {len(top_conf_indices)}\")\n",
    "        continue  # Skip if not enough samples\n",
    "    \n",
    "    # Create training and testing datasets\n",
    "    adata_train = adata_merfish[top_conf_indices].copy()\n",
    "    adata_test = adata_merfish[test_indices].copy()\n",
    "    \n",
    "    # Train and get test loss\n",
    "    test_loss = train_and_evaluate_model(\n",
    "        adata_train, adata_test, label_key='CellType', label_encoder=global_label_encoder,\n",
    "        epoch_num=30, device=device, batch_size=64\n",
    "    )\n",
    "    \n",
    "    # Save the result\n",
    "    high_conf_df = high_conf_df.append({\n",
    "        'Train_Size': T,\n",
    "        'Train_Indices': ','.join(map(str, top_conf_indices)),\n",
    "        'Test_Indices': ','.join(map(str, test_indices)),\n",
    "        'Test_Loss': test_loss\n",
    "    }, ignore_index=True)\n",
    "    \n",
    "    print(f\"Train_Size={T}, Test Loss={test_loss}\")\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "high_conf_df.to_csv('high_confidence_compositions.csv', index=False)\n",
    "\n",
    "print(\"\\nHigh-confidence compositions have been saved to 'high_confidence_compositions.csv'.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Read the results from the CSV files\n",
    "optimal_comp_df = pd.read_csv('optimal_compositions_detailed.csv')\n",
    "high_conf_df = pd.read_csv('high_confidence_compositions.csv')\n",
    "\n",
    "# Calculate the average test loss for each Train_Size in the optimal compositions\n",
    "optimal_loss_df = optimal_comp_df.groupby('Train_Size')['Test_Loss'].mean().reset_index()\n",
    "optimal_loss_df.rename(columns={'Test_Loss': 'Optimal_Test_Loss'}, inplace=True)\n",
    "\n",
    "# Prepare the test loss for the high confidence compositions\n",
    "high_conf_loss_df = high_conf_df[['Train_Size', 'Test_Loss']]\n",
    "high_conf_loss_df.rename(columns={'Test_Loss': 'High_Conf_Test_Loss'}, inplace=True)\n",
    "\n",
    "# Merge the two DataFrames on 'Train_Size'\n",
    "comparison_df = pd.merge(optimal_loss_df, high_conf_loss_df, on='Train_Size')\n",
    "\n",
    "# Sort by Train_Size\n",
    "comparison_df.sort_values('Train_Size', inplace=True)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(comparison_df['Train_Size'], comparison_df['Optimal_Test_Loss'], marker='o', label='Optimal Composition')\n",
    "plt.plot(comparison_df['Train_Size'], comparison_df['High_Conf_Test_Loss'], marker='s', label='High Confidence Composition')\n",
    "\n",
    "plt.title('Comparison of Test Losses by Train Size')\n",
    "plt.xlabel('Train Size')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_16_Nov",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
