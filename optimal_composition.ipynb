{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1187515586.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    from Annotatability import Annotatability_2/models.py\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scanpy as sc\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import roc_auc_score, pairwise_distances, accuracy_score\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "import scipy.sparse as sp\n",
    "from numba import jit\n",
    "from Annotatability import models\n",
    "from Annotatability import metrics\n",
    "import scvi\n",
    "import logging\n",
    "import squidpy as sq\n",
    "import contextlib\n",
    "import io\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # Added imports\n",
    "import scipy.sparse as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.getLogger(\"scvi\").setLevel(logging.WARNING)\n",
    "\n",
    "SMALL_SIZE = 16\n",
    "MEDIUM_SIZE = 20.5\n",
    "BIGGER_SIZE = 24\n",
    "#plt.rcParams[\"font.family\"] = \"Verdana\"\n",
    "plt.rc('font', size=MEDIUM_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "sc.set_figure_params(scanpy=True, fontsize=20.5)\n",
    "\n",
    "# Define custom color palette\n",
    "annotation_order = ['Easy-to-learn', 'Ambiguous', 'Hard-to-learn']\n",
    "annotation_colors = ['green', 'orange', 'red']\n",
    "palette = dict(zip(annotation_order, annotation_colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "# Initialize StringIO object to suppress outputs\n",
    "f = io.StringIO()\n",
    "\n",
    "def train_and_get_prob_list(adata, label_key, epoch_num, device=device, batch_size=128):\n",
    "    print('Training the model...')\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        prob_list = models.follow_training_dyn_neural_net(\n",
    "            adata,\n",
    "            label_key=label_key,\n",
    "            iterNum=epoch_num,\n",
    "            device=device,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "    print('Training complete.')\n",
    "    return prob_list\n",
    "\n",
    "def calculate_confidence_and_variability(prob_list, n_obs, epoch_num):\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        all_conf, all_var = models.probability_list_to_confidence_and_var(\n",
    "            prob_list,\n",
    "            n_obs=n_obs,\n",
    "            epoch_num=epoch_num\n",
    "        )\n",
    "    return all_conf, all_var\n",
    "\n",
    "def find_cutoffs(adata, label_key, device, probability, percentile, epoch_num):\n",
    "    with contextlib.redirect_stdout(f):\n",
    "        cutoff_conf, cutoff_var = models.find_cutoff_paramter(\n",
    "            adata,\n",
    "            label_key,\n",
    "            device,\n",
    "            probability=probability,\n",
    "            percentile=percentile,\n",
    "            epoch_num=epoch_num\n",
    "        )\n",
    "    return cutoff_conf, cutoff_var\n",
    "\n",
    "def assign_annotations(adata, all_conf, all_var, cutoff_conf, cutoff_var, annotation_col='Annotation'):\n",
    "    adata.obs[\"var\"] = all_var.detach().numpy()\n",
    "    adata.obs[\"conf\"] = all_conf.detach().numpy()\n",
    "    adata.obs['conf_binaries'] = pd.Categorical(\n",
    "        (adata.obs['conf'] > cutoff_conf) | (adata.obs['var'] > cutoff_var)\n",
    "    )\n",
    "\n",
    "    annotation_list = []\n",
    "    for i in tqdm(range(adata.n_obs), desc='Assigning annotations'):\n",
    "        if adata.obs['conf_binaries'].iloc[i]:\n",
    "            if (adata.obs['conf'].iloc[i] > 0.95) & (adata.obs['var'].iloc[i] < 0.15):\n",
    "                annotation_list.append('Easy-to-learn')\n",
    "            else:\n",
    "                annotation_list.append('Ambiguous')\n",
    "        else:\n",
    "            annotation_list.append('Hard-to-learn')\n",
    "\n",
    "    adata.obs[annotation_col] = annotation_list\n",
    "    adata.obs['Confidence'] = adata.obs['conf']\n",
    "    adata.obs['Variability'] = adata.obs['var']\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "adata_merfish = sq.datasets.merfish()\n",
    "sc.pp.normalize_per_cell(adata_merfish, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata_merfish)\n",
    "\n",
    "# Map clusters to cell types\n",
    "cell_type_mapping = {\n",
    "    'OD Mature 2': 'OD Mature',\n",
    "    'OD Immature 1': 'OD Immature',\n",
    "    'Inhibitory': 'Inhibitory',\n",
    "    'Excitatory': 'Excitatory',\n",
    "    'Microglia': 'Microglia',\n",
    "    'Astrocyte': 'Astrocyte',\n",
    "    'Endothelial 2': 'Endothelial',\n",
    "    'Endothelial 3': 'Endothelial',\n",
    "    'Endothelial 1': 'Endothelial',\n",
    "    'OD Mature 1': 'OD Mature',\n",
    "    'OD Mature 4': 'OD Mature',\n",
    "    'Pericytes': 'Pericytes',\n",
    "    'OD Mature 3': 'OD Mature',\n",
    "    'Ependymal': 'Ependymal',\n",
    "    'OD Immature 2': 'OD Immature'\n",
    "}\n",
    "adata_merfish.obs['CellType'] = adata_merfish.obs['Cell_class'].map(cell_type_mapping).fillna(adata_merfish.obs['Cell_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_list_merfish_new = train_and_get_prob_list(adata_merfish, label_key='CellType', epoch_num=150, device=device, batch_size=64)\n",
    "\n",
    "# train and get conf and var of merfish\n",
    "all_conf_merfish, all_var_merfish = calculate_confidence_and_variability(prob_list_merfish_new, n_obs=adata_merfish.n_obs, epoch_num=150)\n",
    "\n",
    "# find cutoffs of merdish \n",
    "conf_cutoff, var_cutoff = find_cutoffs(adata_merfish, 'CellType', device, probability=0.1, percentile=90, epoch_num=150)\n",
    "\n",
    "# Assign annotations\n",
    "adata_merfish = assign_annotations(adata_merfish, all_conf_merfish, all_var_merfish, conf_cutoff, var_cutoff, annotation_col='Annotation')\n",
    "\n",
    "# count the number of cells in each group\n",
    "group_counts = adata_merfish.obs['Annotation'].value_counts()\n",
    "\n",
    "print(group_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'group_counts' is a pandas Series with annotations as indices\n",
    "group_counts = adata_merfish.obs['Annotation'].value_counts()\n",
    "\n",
    "# Assign counts to E, A, H\n",
    "E = group_counts.get('Easy-to-learn', 0)\n",
    "A = group_counts.get('Ambiguous', 0)\n",
    "H = group_counts.get('Hard-to-learn', 0)\n",
    "\n",
    "# Get the indices of each group\n",
    "easy_indices = adata_merfish.obs.index[adata_merfish.obs['Annotation'] == 'Easy-to-learn'].tolist()\n",
    "ambiguous_indices = adata_merfish.obs.index[adata_merfish.obs['Annotation'] == 'Ambiguous'].tolist()\n",
    "hard_indices = adata_merfish.obs.index[adata_merfish.obs['Annotation'] == 'Hard-to-learn'].tolist()\n",
    "\n",
    "# Verify the counts match E, A, H\n",
    "print(f\"Number of Easy-to-learn samples: {len(easy_indices)}\")\n",
    "print(f\"Number of Ambiguous samples: {len(ambiguous_indices)}\")\n",
    "print(f\"Number of Hard-to-learn samples: {len(hard_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import scipy.sparse as sp\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Fit LabelEncoder on the entire dataset labels\n",
    "global_label_encoder = LabelEncoder()\n",
    "global_label_encoder.fit(adata_merfish.obs['CellType'])\n",
    "num_classes = len(global_label_encoder.classes_)\n",
    "\n",
    "# Define the neural network class and helper functions\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(BaseNet, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "\n",
    "class Net(BaseNet):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        layer_sizes = [input_size, int(input_size / 2), int(input_size / 4), output_size]\n",
    "        super(Net, self).__init__(layer_sizes)\n",
    "\n",
    "def one_hot_encode(labels, label_encoder):\n",
    "    values = np.array(labels)\n",
    "    integer_encoded = label_encoder.transform(values)\n",
    "    onehot_encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded\n",
    "\n",
    "def is_scipy_cs_sparse(matrix):\n",
    "    return sp.issparse(matrix) and matrix.getformat() == 'csr'\n",
    "\n",
    "# Function to train and evaluate the model on a test set\n",
    "def train_and_evaluate_model(adata_train, adata_test, label_key, label_encoder, epoch_num, device=device, batch_size=128):\n",
    "    # Encode labels using the global label encoder\n",
    "    one_hot_label_train = one_hot_encode(adata_train.obs[label_key], label_encoder=label_encoder)\n",
    "    one_hot_label_test = one_hot_encode(adata_test.obs[label_key], label_encoder=label_encoder)\n",
    "\n",
    "    net = Net(adata_train.X.shape[1], output_size=num_classes)\n",
    "    net.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Prepare training data\n",
    "    if is_scipy_cs_sparse(adata_train.X):\n",
    "        x_train = adata_train.X.toarray()\n",
    "    else:\n",
    "        x_train = np.array(adata_train.X)\n",
    "    tensor_x_train = torch.Tensor(x_train).to(device)\n",
    "    tensor_y_train = torch.LongTensor(np.argmax(one_hot_label_train, axis=1)).to(device)\n",
    "    train_dataset = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Prepare test data\n",
    "    if is_scipy_cs_sparse(adata_test.X):\n",
    "        x_test = adata_test.X.toarray()\n",
    "    else:\n",
    "        x_test = np.array(adata_test.X)\n",
    "    tensor_x_test = torch.Tensor(x_test).to(device)\n",
    "    tensor_y_test = torch.LongTensor(np.argmax(one_hot_label_test, axis=1)).to(device)\n",
    "\n",
    "    # Train the network\n",
    "    net.train()\n",
    "    for epoch in range(epoch_num):\n",
    "        for inputs, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on test set\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = net(tensor_x_test)\n",
    "        test_loss = criterion(outputs, tensor_y_test).item()\n",
    "\n",
    "    return test_loss\n",
    "\n",
    "# Load existing results from CSV or create an empty DataFrame\n",
    "csv_file = 'best_compositions.csv'\n",
    "try:\n",
    "    results_df = pd.read_csv(csv_file)\n",
    "except FileNotFoundError:\n",
    "    results_df = pd.DataFrame(columns=['Train_Size', 'Easy', 'Ambiguous', 'Hard', 'Test_Loss'])\n",
    "\n",
    "# Define the number of repeats per train size\n",
    "repeats_per_size = 10  # Set this to your desired number of repeats\n",
    "\n",
    "# Convert the 'Train_Size' column to a dictionary with counts for faster lookup\n",
    "existing_counts = results_df['Train_Size'].value_counts().to_dict()\n",
    "\n",
    "# Now, for dataset sizes (train sizes)\n",
    "train_sizes = [500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500, 2750, 3000]\n",
    "best_compositions = {}\n",
    "\n",
    "for T in train_sizes:\n",
    "    current_runs = existing_counts.get(T, 0)\n",
    "    runs_needed = repeats_per_size - current_runs\n",
    "\n",
    "    if runs_needed <= 0:\n",
    "        # Use existing entries\n",
    "        existing_rows = results_df[results_df['Train_Size'] == T]\n",
    "        for idx, row in existing_rows.iterrows():\n",
    "            easy = row['Easy']\n",
    "            ambiguous = row['Ambiguous']\n",
    "            hard = row['Hard']\n",
    "            test_loss = row['Test_Loss']\n",
    "            print(\n",
    "                f\"Using cached result for Train_Size={T}: Easy={easy}, Ambiguous={ambiguous}, Hard={hard}, Test Loss={test_loss}\"\n",
    "            )\n",
    "            # You can choose how to store multiple runs. Here, we store all runs in a list.\n",
    "            if T not in best_compositions:\n",
    "                best_compositions[T] = []\n",
    "            best_compositions[T].append({'composition': (easy, ambiguous, hard), 'Test_Loss': test_loss})\n",
    "        continue  # Skip computation for this T as all repeats are already done\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nProcessing train dataset size: {T} (Run {current_runs + 1} to {repeats_per_size})\")\n",
    "\n",
    "        for run in range(current_runs + 1, repeats_per_size + 1):\n",
    "            print(f\"\\n--- Run {run} for Train_Size={T} ---\")\n",
    "\n",
    "            # Calculate test size (25% of train size)\n",
    "            test_size = int(0.25 * T)\n",
    "            total_size = T + test_size\n",
    "            print(f\"Total dataset size (Train + Test): {total_size} (Train: {T}, Test: {test_size})\")\n",
    "\n",
    "            # Define step size as a function of T\n",
    "            step_size = max(1, T // 100)\n",
    "\n",
    "            # Generate compositions summing up to T (train size)\n",
    "            compositions = []\n",
    "            for e in range(0, min(T, E) + 1, step_size):\n",
    "                for a in range(0, min(T - e, A) + 1, step_size):\n",
    "                    h = T - e - a\n",
    "                    if h >= 0 and h <= H:\n",
    "                        compositions.append((e, a, h))\n",
    "            if not compositions:\n",
    "                print(f\"No valid compositions for Train Size={T}\")\n",
    "                # Save an entry indicating no valid compositions\n",
    "                new_row = {'Train_Size': T, 'Easy': None, 'Ambiguous': None, 'Hard': None, 'Test_Loss': None}\n",
    "                # Replace append with pd.concat\n",
    "                new_row_df = pd.DataFrame([new_row])\n",
    "                results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "                results_df.to_csv(csv_file, index=False)\n",
    "                continue\n",
    "\n",
    "            print(f\"Total compositions for Train Size={T}: {len(compositions)}\")\n",
    "\n",
    "            min_loss = float('inf')\n",
    "            best_comp = None\n",
    "\n",
    "            # For each composition, train and get test loss\n",
    "            for comp in tqdm(compositions, desc=f\"Testing compositions for Train Size={T} - Run {run}\"):\n",
    "                e, a, h = comp\n",
    "                # Ensure not exceeding group counts\n",
    "                if e > E or a > A or h > H:\n",
    "                    continue  # Invalid composition\n",
    "\n",
    "                # Ensure we have enough samples in each group\n",
    "                if len(easy_indices) < e or len(ambiguous_indices) < a or len(hard_indices) < h:\n",
    "                    continue  # Skip if not enough samples\n",
    "\n",
    "                # Randomly sample e, a, h samples from each group for training\n",
    "                train_easy_indices = random.sample(easy_indices, e) if e > 0 else []\n",
    "                train_ambiguous_indices = random.sample(ambiguous_indices, a) if a > 0 else []\n",
    "                train_hard_indices = random.sample(hard_indices, h) if h > 0 else []\n",
    "                train_indices = train_easy_indices + train_ambiguous_indices + train_hard_indices\n",
    "\n",
    "                # Ensure total train samples equal T\n",
    "                if len(train_indices) != T:\n",
    "                    continue  # Skip if train size mismatch\n",
    "\n",
    "                # All indices excluding the ones in train_indices\n",
    "                remaining_indices = list(set(adata_merfish.obs.index) - set(train_indices))\n",
    "\n",
    "                # Ensure we have enough remaining samples for test set\n",
    "                if len(remaining_indices) < test_size:\n",
    "                    continue  # Skip if not enough samples\n",
    "\n",
    "                # Randomly sample test_size samples from remaining_indices\n",
    "                test_indices = random.sample(remaining_indices, test_size)\n",
    "\n",
    "                # Create training and testing datasets\n",
    "                adata_train = adata_merfish[train_indices].copy()\n",
    "                adata_test = adata_merfish[test_indices].copy()\n",
    "\n",
    "                # Train and get test loss\n",
    "                test_loss = train_and_evaluate_model(\n",
    "                    adata_train, adata_test, label_key='CellType', label_encoder=global_label_encoder,\n",
    "                    epoch_num=30, device=device, batch_size=64\n",
    "                )\n",
    "                # Update minimum loss and best composition\n",
    "                if test_loss < min_loss:\n",
    "                    min_loss = test_loss\n",
    "                    best_comp = comp\n",
    "\n",
    "            if best_comp is not None:\n",
    "                easy, ambiguous, hard = best_comp\n",
    "                print(\n",
    "                    f\"Best composition for Train_Size={T} (Run {run}): Easy={easy}, Ambiguous={ambiguous}, Hard={hard}, Test Loss={min_loss}\"\n",
    "                )\n",
    "                # Append to best_compositions\n",
    "                if T not in best_compositions:\n",
    "                    best_compositions[T] = []\n",
    "                best_compositions[T].append({'composition': best_comp, 'Test_Loss': min_loss})\n",
    "                # Save the result to the DataFrame and CSV\n",
    "                new_row = {\n",
    "                    'Train_Size': T,\n",
    "                    'Easy': easy,\n",
    "                    'Ambiguous': ambiguous,\n",
    "                    'Hard': hard,\n",
    "                    'Test_Loss': min_loss\n",
    "                }\n",
    "                # Replace append with pd.concat\n",
    "                new_row_df = pd.DataFrame([new_row])\n",
    "                results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "                results_df.to_csv(csv_file, index=False)\n",
    "            else:\n",
    "                print(f\"No valid compositions found for Train_Size={T} (Run {run})\")\n",
    "                # Save an entry indicating no valid compositions\n",
    "                new_row = {'Train_Size': T, 'Easy': None, 'Ambiguous': None, 'Hard': None, 'Test_Loss': None}\n",
    "                # Replace append with pd.concat\n",
    "                new_row_df = pd.DataFrame([new_row])\n",
    "                results_df = pd.concat([results_df, new_row_df], ignore_index=True)\n",
    "                results_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the compositions from the CSV file\n",
    "csv_file = 'best_compositions.csv'\n",
    "try:\n",
    "    results_df = pd.read_csv(csv_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"CSV file '{csv_file}' not found.\")\n",
    "    results_df = pd.DataFrame(columns=['Train_Size', 'Easy', 'Ambiguous', 'Hard', 'Test_Loss'])\n",
    "\n",
    "# Filter out rows with missing compositions\n",
    "results_df = results_df.dropna(subset=['Easy', 'Ambiguous', 'Hard'])\n",
    "\n",
    "# Convert counts to floats and Train_Size to int\n",
    "results_df['Easy'] = results_df['Easy'].astype(float)\n",
    "results_df['Ambiguous'] = results_df['Ambiguous'].astype(float)\n",
    "results_df['Hard'] = results_df['Hard'].astype(float)\n",
    "results_df['Train_Size'] = results_df['Train_Size'].astype(int)\n",
    "\n",
    "# Check if all train sizes have the same number of runs\n",
    "counts_per_size = results_df['Train_Size'].value_counts()\n",
    "if counts_per_size.nunique() != 1:\n",
    "    print(\"Warning: Not all train sizes have the same number of rows in the CSV for each train size.\")\n",
    "\n",
    "# Calculate total and proportions for each row\n",
    "results_df['Total'] = results_df['Easy'] + results_df['Ambiguous'] + results_df['Hard']\n",
    "results_df['Proportion_Easy'] = results_df['Easy'] / results_df['Total']\n",
    "results_df['Proportion_Ambiguous'] = results_df['Ambiguous'] / results_df['Total']\n",
    "results_df['Proportion_Hard'] = results_df['Hard'] / results_df['Total']\n",
    "\n",
    "# Group by Train_Size and calculate mean and standard deviation of proportions\n",
    "grouped = results_df.groupby('Train_Size').agg({\n",
    "    'Proportion_Easy': ['mean', 'std'],\n",
    "    'Proportion_Ambiguous': ['mean', 'std'],\n",
    "    'Proportion_Hard': ['mean', 'std']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "grouped.columns = ['Train_Size',\n",
    "                   'Proportion_Easy_mean', 'Proportion_Easy_std',\n",
    "                   'Proportion_Ambiguous_mean', 'Proportion_Ambiguous_std',\n",
    "                   'Proportion_Hard_mean', 'Proportion_Hard_std']\n",
    "\n",
    "# Ensure that the mean proportions sum to 1 (optional assertion)\n",
    "assert np.allclose(grouped[['Proportion_Easy_mean', 'Proportion_Ambiguous_mean', 'Proportion_Hard_mean']].sum(axis=1), 1), \"Mean proportions do not sum to 1.\"\n",
    "\n",
    "# Prepare data for plotting\n",
    "train_sizes = grouped['Train_Size'].values\n",
    "proportion_e_mean = grouped['Proportion_Easy_mean'].values\n",
    "proportion_a_mean = grouped['Proportion_Ambiguous_mean'].values\n",
    "proportion_h_mean = grouped['Proportion_Hard_mean'].values\n",
    "proportion_e_std = grouped['Proportion_Easy_std'].values\n",
    "proportion_a_std = grouped['Proportion_Ambiguous_std'].values\n",
    "proportion_h_std = grouped['Proportion_Hard_std'].values\n",
    "\n",
    "# Verify that all arrays have the same length\n",
    "array_lengths = [len(train_sizes), len(proportion_e_mean), len(proportion_a_mean), len(proportion_h_mean),\n",
    "                len(proportion_e_std), len(proportion_a_std), len(proportion_h_std)]\n",
    "if len(set(array_lengths)) != 1:\n",
    "    raise ValueError(f\"Array length mismatch: {array_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting grouped bar chart with error bars (variance) without percentage labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for plotting\n",
    "# (Assuming 'grouped' DataFrame is already prepared in the preprocessing cell)\n",
    "train_sizes = grouped['Train_Size'].values\n",
    "proportion_e_mean = grouped['Proportion_Easy_mean'].values\n",
    "proportion_a_mean = grouped['Proportion_Ambiguous_mean'].values\n",
    "proportion_h_mean = grouped['Proportion_Hard_mean'].values\n",
    "proportion_e_std = grouped['Proportion_Easy_std'].values\n",
    "proportion_a_std = grouped['Proportion_Ambiguous_std'].values\n",
    "proportion_h_std = grouped['Proportion_Hard_std'].values\n",
    "\n",
    "# Set up the plot for Grouped Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "bar_width = 0.25\n",
    "index = np.arange(len(train_sizes))\n",
    "\n",
    "# Plot each category as separate bars with error bars\n",
    "bars_easy = ax.bar(index - bar_width, proportion_e_mean, bar_width, yerr=proportion_e_std,\n",
    "                  label='Easy-to-learn', color='green', capsize=5)\n",
    "bars_ambiguous = ax.bar(index, proportion_a_mean, bar_width, yerr=proportion_a_std,\n",
    "                        label='Ambiguous', color='orange', capsize=5)\n",
    "bars_hard = ax.bar(index + bar_width, proportion_h_mean, bar_width, yerr=proportion_h_std,\n",
    "                  label='Hard-to-learn', color='red', capsize=5)\n",
    "\n",
    "# Customize the axes\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([str(size) for size in train_sizes], rotation=45)\n",
    "ax.set_ylabel('Average Proportion')\n",
    "ax.set_xlabel('Train Set Size')\n",
    "ax.set_title('Optimal Composition of Train Set Samples with Standard Deviation')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting stacked bar chart without error bars but with percentage labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for plotting\n",
    "# (Assuming 'grouped' DataFrame is already prepared in the preprocessing cell)\n",
    "# train_sizes, proportion_e_mean, proportion_a_mean, proportion_h_mean are already defined\n",
    "\n",
    "# Set up the plot for Stacked Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "bar_width = 0.6\n",
    "index = np.arange(len(train_sizes))\n",
    "\n",
    "# Plot Easy-to-learn segment\n",
    "bars_easy = ax.bar(index, proportion_e_mean, bar_width, label='Easy-to-learn', color='green')\n",
    "\n",
    "# Plot Ambiguous segment on top of Easy-to-learn\n",
    "bars_ambiguous = ax.bar(index, proportion_a_mean, bar_width, bottom=proportion_e_mean,\n",
    "                        label='Ambiguous', color='orange')\n",
    "\n",
    "# Plot Hard-to-learn segment on top of Ambiguous\n",
    "bars_hard = ax.bar(index, proportion_h_mean, bar_width, bottom=proportion_e_mean + proportion_a_mean,\n",
    "                  label='Hard-to-learn', color='red')\n",
    "\n",
    "# Add percentage labels\n",
    "for i in range(len(train_sizes)):\n",
    "    # Easy-to-learn\n",
    "    if proportion_e_mean[i] > 0.05:\n",
    "        ax.text(index[i], proportion_e_mean[i]/2,\n",
    "                f\"{proportion_e_mean[i]*100:.1f}%\", ha='center', va='center',\n",
    "                color='white', fontsize=10)\n",
    "    # Ambiguous\n",
    "    if proportion_a_mean[i] > 0.05:\n",
    "        ax.text(index[i], proportion_e_mean[i] + proportion_a_mean[i]/2,\n",
    "                f\"{proportion_a_mean[i]*100:.1f}%\", ha='center', va='center',\n",
    "                color='white', fontsize=10)\n",
    "    # Hard-to-learn\n",
    "    if proportion_h_mean[i] > 0.05:\n",
    "        ax.text(index[i], proportion_e_mean[i] + proportion_a_mean[i] + proportion_h_mean[i]/2,\n",
    "                f\"{proportion_h_mean[i]*100:.1f}%\", ha='center', va='center',\n",
    "                color='white', fontsize=10)\n",
    "\n",
    "# Customize the axes\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels([str(size) for size in train_sizes], rotation=45)\n",
    "ax.set_ylabel('Average Proportion')\n",
    "ax.set_xlabel('Train Set Size')\n",
    "ax.set_title('Optimal Composition of Train Set Samples')\n",
    "ax.legend(loc='lower left')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
